{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cd0402f",
   "metadata": {},
   "source": [
    "# Hello World"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c55cc3",
   "metadata": {},
   "source": [
    "## Simple invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c7b27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Agora em portuguÃªs: Adoro programaÃ§Ã£o.', additional_kwargs={}, response_metadata={'model': 'deepseek-v3.1:671b-cloud', 'created_at': '2025-11-20T22:19:14.015873942Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1049134742, 'load_duration': None, 'prompt_eval_count': 36, 'prompt_eval_duration': None, 'eval_count': 14, 'eval_duration': None, 'model_name': 'deepseek-v3.1:671b-cloud', 'model_provider': 'ollama'}, id='lc_run--b38da2ae-da16-49a3-81b8-1afa6aa0e0fb-0', usage_metadata={'input_tokens': 36, 'output_tokens': 14, 'total_tokens': 50})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"deepseek-v3.1:671b-cloud\",\n",
    "    validate_model_on_init=True,\n",
    "    temperature=0.8,\n",
    "    num_predict=256,\n",
    "    # other params ...\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    (\"system\", \"You are a helpful translator. Translate the user sentence to French.\"),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "    (\"ai\", \"J'adore la programmation.\"),\n",
    "    (\"human\", \"Now in portuguese\")\n",
    "\n",
    "]\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d95ef86",
   "metadata": {},
   "source": [
    "## Streaming invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0adf0412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the words \"Hello World!\" separated individually:\n",
      "\n",
      "**Hello**  \n",
      "**World!**"
     ]
    }
   ],
   "source": [
    "for chunk in model.stream(\"Return the words Hello World! separatedly.\"):\n",
    "    print(chunk.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751293f8",
   "metadata": {},
   "source": [
    "## Asynchronous invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fed4012c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n",
      "!\n",
      " ðŸ˜Š\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await model.ainvoke(\"Hello how are you!\")\n",
    "\n",
    "async for chunk in model.astream(\"Say hello world!\"):\n",
    "    print(chunk.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaed9d4b",
   "metadata": {},
   "source": [
    "## Batch invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b23e754e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Hello, world! ðŸŒ', additional_kwargs={}, response_metadata={'model': 'deepseek-v3.1:671b-cloud', 'created_at': '2025-11-01T23:39:00.883273419Z', 'done': True, 'done_reason': 'stop', 'total_duration': 651696330, 'load_duration': None, 'prompt_eval_count': 10, 'prompt_eval_duration': None, 'eval_count': 7, 'eval_duration': None, 'model_name': 'deepseek-v3.1:671b-cloud', 'model_provider': 'ollama'}, id='lc_run--823925dc-9c18-4534-af3d-41f44c88dfb1-0', usage_metadata={'input_tokens': 10, 'output_tokens': 7, 'total_tokens': 17}),\n",
       " AIMessage(content='Goodbye, world! ðŸ‘‹  \\nIf you have any more questions or need help in the future, Iâ€™m here for you. Take care! ðŸ˜Š', additional_kwargs={}, response_metadata={'model': 'deepseek-v3.1:671b-cloud', 'created_at': '2025-11-01T23:39:01.207991262Z', 'done': True, 'done_reason': 'stop', 'total_duration': 963045650, 'load_duration': None, 'prompt_eval_count': 10, 'prompt_eval_duration': None, 'eval_count': 34, 'eval_duration': None, 'model_name': 'deepseek-v3.1:671b-cloud', 'model_provider': 'ollama'}, id='lc_run--e541db5c-3d68-480d-b9a7-6897f35635c2-0', usage_metadata={'input_tokens': 10, 'output_tokens': 34, 'total_tokens': 44})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [(\"human\", \"Say hello world!\"), (\"human\", \"Say goodbye world!\")]\n",
    "await model.abatch(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cabd51",
   "metadata": {},
   "source": [
    "## Json format response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d5ba22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"location\": \"Kyoto\",\n",
      "  \"time_of_day\": \"sunset\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "json_model = ChatOllama(model=\"deepseek-v3.1:671b-cloud\", format=\"json\")\n",
    "res = json_model.invoke(\n",
    "    \"Return a query for the weather in a random location and time of day with two keys: location and time_of_day. \"\n",
    "    \"Respond using JSON only.\"\n",
    ")\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1829f238",
   "metadata": {},
   "source": [
    "## Tool use example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "597f43a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'validate_user', 'args': {'user_id': 123}, 'id': 'cab96f60-f45c-49be-95f7-11a6dde59f08', 'type': 'tool_call'}]\n",
      "content=\"I'll validate user 123 for you using the available validation tool.\" additional_kwargs={} response_metadata={'model': 'deepseek-v3.1:671b-cloud', 'created_at': '2025-11-20T22:39:25.229244596Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1037306492, 'load_duration': None, 'prompt_eval_count': 218, 'prompt_eval_duration': None, 'eval_count': 29, 'eval_duration': None, 'model_name': 'deepseek-v3.1:671b-cloud', 'model_provider': 'ollama'} id='lc_run--7cbcef5e-9929-4a55-8463-d8590e9281ea-0' tool_calls=[{'name': 'validate_user', 'args': {'user_id': 123}, 'id': 'cab96f60-f45c-49be-95f7-11a6dde59f08', 'type': 'tool_call'}] usage_metadata={'input_tokens': 218, 'output_tokens': 29, 'total_tokens': 247}\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def validate_user(user_id: int) -> bool:\n",
    "    \"\"\"Validate user using historical addresses.\n",
    "\n",
    "    Args:\n",
    "        user_id (int): the user ID.\n",
    "    Returns:\n",
    "        bool: whether the user is valid or not.\n",
    "    \"\"\"\n",
    "    return user_id == 42\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"deepseek-v3.1:671b-cloud\",\n",
    "    temperature=0,\n",
    ").bind_tools([validate_user])\n",
    "\n",
    "ans = model.invoke(\"Could you validate user 123? They previously lived at \"\n",
    "    \"123 Fake St in Boston MA and 234 Pretend Boulevard in \"\n",
    "    \"Houston TX. Tell me if they are valid.\"\n",
    "    \"Tell me if the user is valid or not.\"\n",
    ")\n",
    "\n",
    "print(ans.tool_calls)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6cbfc4d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'validate_user',\n",
       "  'args': {'user_id': 123},\n",
       "  'id': 'cab96f60-f45c-49be-95f7-11a6dde59f08',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b094470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll validate user 123 for you using the available validation tool.\n"
     ]
    }
   ],
   "source": [
    "print(ans.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c269978f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'll validate user 123 for you using the available validation tool.\", additional_kwargs={}, response_metadata={'model': 'deepseek-v3.1:671b-cloud', 'created_at': '2025-11-20T22:39:25.229244596Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1037306492, 'load_duration': None, 'prompt_eval_count': 218, 'prompt_eval_duration': None, 'eval_count': 29, 'eval_duration': None, 'model_name': 'deepseek-v3.1:671b-cloud', 'model_provider': 'ollama'}, id='lc_run--7cbcef5e-9929-4a55-8463-d8590e9281ea-0', tool_calls=[{'name': 'validate_user', 'args': {'user_id': 123}, 'id': 'cab96f60-f45c-49be-95f7-11a6dde59f08', 'type': 'tool_call'}], usage_metadata={'input_tokens': 218, 'output_tokens': 29, 'total_tokens': 247})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a02e995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'validate_user', 'args': {'addresses': ['123 Fake St in Boston MA', '234 Pretend Boulevard in Houston TX'], 'user_id': 123}, 'id': '73462222-ecc7-4516-a3e4-51584ed43903', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import AIMessage\n",
    "\n",
    "if isinstance(ans, AIMessage) and ans.tool_calls:\n",
    "    print(ans.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b05f98a",
   "metadata": {},
   "source": [
    "## Prompt templates example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04a426e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Answer: Natural language processing' additional_kwargs={} response_metadata={'model': 'deepseek-v3.1:671b-cloud', 'created_at': '2025-11-20T22:41:49.688801842Z', 'done': True, 'done_reason': 'stop', 'total_duration': 783003571, 'load_duration': None, 'prompt_eval_count': 71, 'prompt_eval_duration': None, 'eval_count': 6, 'eval_duration': None, 'model_name': 'deepseek-v3.1:671b-cloud', 'model_provider': 'ollama'} id='lc_run--bc2050a9-bb37-4c0b-aeed-a623bb67f98b-0' usage_metadata={'input_tokens': 71, 'output_tokens': 6, 'total_tokens': 77}\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"\"\"Answer the question based on the context below.\n",
    "    If the question can't be answered using the context, say \"I don't know\".\n",
    "\n",
    "    Context: {context}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Answer:\"\"\")\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"deepseek-v3.1:671b-cloud\",\n",
    ")\n",
    "\n",
    "prompt = template.invoke({\n",
    "    \"context\": \"The most recent advancements in AI technology have led to significant improvements in natural language processing.\",\n",
    "    \"question\": \"What field has seen significant improvements due to recent advancements in AI?\"\n",
    "})\n",
    "\n",
    "completion = model.invoke(prompt)\n",
    "\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f1f4a87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question based on the context below.\n",
      "    If the question can't be answered using the context, say \"I don't know\".\n",
      "\n",
      "    Context: The most recent advancements in AI technology have led to significant improvements in natural language processing.\n",
      "\n",
      "    Question: What field has seen significant improvements due to recent advancements in AI?\n",
      "\n",
      "    Answer:\n"
     ]
    }
   ],
   "source": [
    "print(prompt.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1139ecae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Natural language processing\n"
     ]
    }
   ],
   "source": [
    "print(completion.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c449d70",
   "metadata": {},
   "source": [
    "## Chain example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3b63fc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"answer\": \"They weigh the same: one pound each.\",\n",
      "  \"justification\": \"A pound is a unit of weight; regardless of the material, a pound of feathers and a pound of bricks each have a mass of one pound, so their weights are equal.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that provides answers with justifications. You must respond in JSON format using the specified keys.\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"gpt-oss:120b-cloud\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "parser = JsonOutputParser(keys=[\"answer\", \"justification\"])\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\"question\": \"\"\"What weight more, a pound of feathers or a pound of bricks? (use plain text)\"\"\"}\n",
    ")\n",
    "\n",
    "import json\n",
    "\n",
    "print(json.dumps(response, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
