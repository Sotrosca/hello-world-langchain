{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cd0402f",
   "metadata": {},
   "source": [
    "# Hello World"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c55cc3",
   "metadata": {},
   "source": [
    "## Simple invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9c7b27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\facun\\anaconda3\\envs\\langchain\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, response_metadata={'model': 'deepseek-v3.1:671b-cloud', 'created_at': '2025-11-16T20:45:50.261373914Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1032841066, 'load_duration': None, 'prompt_eval_count': 21, 'prompt_eval_duration': None, 'eval_count': 8, 'eval_duration': None, 'model_name': 'deepseek-v3.1:671b-cloud', 'model_provider': 'ollama'}, id='lc_run--ac876085-9f49-4661-bba9-e2005cb5c774-0', usage_metadata={'input_tokens': 21, 'output_tokens': 8, 'total_tokens': 29})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"deepseek-v3.1:671b-cloud\",\n",
    "    validate_model_on_init=True,\n",
    "    temperature=0.8,\n",
    "    num_predict=256,\n",
    "    # other params ...\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    (\"system\", \"You are a helpful translator. Translate the user sentence to French.\"),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d95ef86",
   "metadata": {},
   "source": [
    "## Streaming invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0adf0412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the words \"Hello World!\" separated individually:\n",
      "\n",
      "**Hello**  \n",
      "**World!**"
     ]
    }
   ],
   "source": [
    "for chunk in model.stream(\"Return the words Hello World! separatedly.\"):\n",
    "    print(chunk.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751293f8",
   "metadata": {},
   "source": [
    "## Asynchronous invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fed4012c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n",
      "!\n",
      " ðŸ˜Š\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await model.ainvoke(\"Hello how are you!\")\n",
    "\n",
    "async for chunk in model.astream(\"Say hello world!\"):\n",
    "    print(chunk.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaed9d4b",
   "metadata": {},
   "source": [
    "## Batch invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b23e754e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Hello, world! ðŸŒ', additional_kwargs={}, response_metadata={'model': 'deepseek-v3.1:671b-cloud', 'created_at': '2025-11-01T23:39:00.883273419Z', 'done': True, 'done_reason': 'stop', 'total_duration': 651696330, 'load_duration': None, 'prompt_eval_count': 10, 'prompt_eval_duration': None, 'eval_count': 7, 'eval_duration': None, 'model_name': 'deepseek-v3.1:671b-cloud', 'model_provider': 'ollama'}, id='lc_run--823925dc-9c18-4534-af3d-41f44c88dfb1-0', usage_metadata={'input_tokens': 10, 'output_tokens': 7, 'total_tokens': 17}),\n",
       " AIMessage(content='Goodbye, world! ðŸ‘‹  \\nIf you have any more questions or need help in the future, Iâ€™m here for you. Take care! ðŸ˜Š', additional_kwargs={}, response_metadata={'model': 'deepseek-v3.1:671b-cloud', 'created_at': '2025-11-01T23:39:01.207991262Z', 'done': True, 'done_reason': 'stop', 'total_duration': 963045650, 'load_duration': None, 'prompt_eval_count': 10, 'prompt_eval_duration': None, 'eval_count': 34, 'eval_duration': None, 'model_name': 'deepseek-v3.1:671b-cloud', 'model_provider': 'ollama'}, id='lc_run--e541db5c-3d68-480d-b9a7-6897f35635c2-0', usage_metadata={'input_tokens': 10, 'output_tokens': 34, 'total_tokens': 44})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [(\"human\", \"Say hello world!\"), (\"human\", \"Say goodbye world!\")]\n",
    "await model.abatch(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cabd51",
   "metadata": {},
   "source": [
    "## Json format response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d5ba22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"location\": \"Kyoto\",\n",
      "  \"time_of_day\": \"sunset\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "json_model = ChatOllama(model=\"deepseek-v3.1:671b-cloud\", format=\"json\")\n",
    "res = json_model.invoke(\n",
    "    \"Return a query for the weather in a random location and time of day with two keys: location and time_of_day. \"\n",
    "    \"Respond using JSON only.\"\n",
    ")\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1829f238",
   "metadata": {},
   "source": [
    "## Tool use example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "597f43a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "content='I can help you validate user 123 using their previous addresses. However, I need to clarify something about the address format you provided.\\n\\nYou mentioned \"123 Fake St in Boston MA\" and \"234 Pretend Boulevard in Houston TX\" - should I use these addresses exactly as written, or would you prefer them formatted in a more standard address format (like \"123 Fake St, Boston, MA\" and \"234 Pretend Boulevard, Houston, TX\")?\\n\\nOnce you confirm the preferred format, I\\'ll proceed with the validation using the validate_user function.' additional_kwargs={} response_metadata={'model': 'deepseek-v3.1:671b-cloud', 'created_at': '2025-11-01T23:39:06.907002371Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3626433136, 'load_duration': None, 'prompt_eval_count': 233, 'prompt_eval_duration': None, 'eval_count': 112, 'eval_duration': None, 'model_name': 'deepseek-v3.1:671b-cloud', 'model_provider': 'ollama'} id='lc_run--bdf0d43f-438c-4104-ae1d-33a5497ed772-0' usage_metadata={'input_tokens': 233, 'output_tokens': 112, 'total_tokens': 345}\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def validate_user(user_id: int, addresses: List[str]) -> bool:\n",
    "    \"\"\"Validate user using historical addresses.\n",
    "\n",
    "    Args:\n",
    "        user_id (int): the user ID.\n",
    "        addresses (List[str]): Previous addresses as a list of strings.\n",
    "    \"\"\"\n",
    "    return user_id == 42\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"deepseek-v3.1:671b-cloud\",\n",
    "    temperature=0,\n",
    ").bind_tools([validate_user])\n",
    "\n",
    "ans = model.invoke(\"Could you validate user 123? They previously lived at \"\n",
    "    \"123 Fake St in Boston MA and 234 Pretend Boulevard in \"\n",
    "    \"Houston TX. Tell me if they are valid.\"\n",
    ")\n",
    "\n",
    "print(ans.tool_calls)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a02e995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import AIMessage\n",
    "\n",
    "if isinstance(ans, AIMessage) and ans.tool_calls:\n",
    "    print(ans.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b05f98a",
   "metadata": {},
   "source": [
    "## Prompt templates example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04a426e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"I don't know.\" additional_kwargs={} response_metadata={'model': 'deepseek-v3.1:671b-cloud', 'created_at': '2025-11-01T23:39:07.910810382Z', 'done': True, 'done_reason': 'stop', 'total_duration': 596445556, 'load_duration': None, 'prompt_eval_count': 65, 'prompt_eval_duration': None, 'eval_count': 6, 'eval_duration': None, 'model_name': 'deepseek-v3.1:671b-cloud', 'model_provider': 'ollama'} id='lc_run--9a47ef06-cf4f-4986-b99c-6251f53012ed-0' usage_metadata={'input_tokens': 65, 'output_tokens': 6, 'total_tokens': 71}\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"\"\"Answer the question based on the context below.\n",
    "    If the question can't be answered using the context, say \"I don't know\".\n",
    "\n",
    "    Context: {context}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Answer:\"\"\")\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"deepseek-v3.1:671b-cloud\",)\n",
    "\n",
    "prompt = template.invoke({\n",
    "    \"context\": \"The most recent advancements in AI technology have led to significant improvements in natural language processing.\",\n",
    "    \"question\": \"Which model providers offer LLMs?\"\n",
    "})\n",
    "\n",
    "completion = model.invoke(prompt)\n",
    "\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c449d70",
   "metadata": {},
   "source": [
    "## Chain example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b63fc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"answer\": \"They weigh the same.\",\n",
      "  \"justification\": \"A pound is a unit of weight. Whether the material is feathers or bricks, one pound of each contains the same weight of 1 pound.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that provides answers with justifications. You must respond in JSON format using the specified keys.\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"gpt-oss:120b-cloud\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "parser = JsonOutputParser(keys=[\"answer\", \"justification\"])\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\"question\": \"\"\"What weight more, a pound of feathers or a pound of bricks? (use plain text)\"\"\"}\n",
    ")\n",
    "\n",
    "import json\n",
    "\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c604743",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
